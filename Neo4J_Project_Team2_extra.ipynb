{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63e2d0f",
   "metadata": {},
   "source": [
    "Python version 3.9.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34ab42",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8484c",
   "metadata": {},
   "source": [
    "Here we load all the imports we need to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b27e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.bulk import create_nodes\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from py2neo.matching import *\n",
    "from py2neo import Graph\n",
    "from py2neo.bulk import create_relationships\n",
    "from itertools import islice\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775d7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a0638",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad3d57",
   "metadata": {},
   "source": [
    "This function will help us later.\n",
    "If we have a dataframe column that consists of lists.\n",
    "Imagine a row that has an id value in the fist column and in the second column it has lists\n",
    "Lets say id would be 1 and the list would be [cat,dog]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b6432",
   "metadata": {},
   "source": [
    "After the unnest function we would get 2 rows that would be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769c128",
   "metadata": {},
   "source": [
    "Row1 : 1 [cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7ff9d",
   "metadata": {},
   "source": [
    "Row2 : 1 [dog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f8ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest(df, col, reset_index=False):\n",
    "    import pandas as pd\n",
    "    col_flat = pd.DataFrame([[i, x] \n",
    "                       for i, y in df[col].apply(list).iteritems() \n",
    "                           for x in y], columns=['I', col])\n",
    "    col_flat = col_flat.set_index('I')\n",
    "    df = df.drop(col, 1)\n",
    "    df = df.merge(col_flat, left_index=True, right_index=True)\n",
    "    if reset_index:\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f4735",
   "metadata": {},
   "source": [
    "# Load File and Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08edb18",
   "metadata": {},
   "source": [
    "Here we create the graph, setup the options that our dataframe will have and load the dataset from the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdff02d",
   "metadata": {},
   "source": [
    "Notice that we drop duplicates based on the id_str column. We do that because each tweet is supposed to have a unique id, either it is tweet or a retweet. So, we assume that the duplicate is a fault and we drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a99a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "tweets = pd.read_csv(r'C:\\Users\\xaris\\Desktop\\webmining\\TwitterData.csv')\n",
    "tweets = tweets.drop_duplicates(subset=['id_str']).reset_index(drop=True)\n",
    "tweets = tweets.dropna(subset = ['id_str'])\n",
    "g = Graph(password='0000')\n",
    "graph = Graph(password='0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8c557",
   "metadata": {},
   "source": [
    "# Tweet Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2703c",
   "metadata": {},
   "source": [
    "We create 2 dataframes: one for the normal tweets (original_tweets) and one for the retweeted tweets (retweeted_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82a8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweets = tweets[tweets['retweeted_status.id_str'].isnull()]\n",
    "retweeted_tweets = tweets.dropna(subset = ['retweeted_status.id_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b43f8c",
   "metadata": {},
   "source": [
    "We now choose what columns we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccd3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweets = original_tweets[['id_str',\n",
    "                                   'user.id_str',\n",
    "                                   'created_at',\n",
    "                                   'source',\n",
    "                                   'text',\n",
    "                                   'entities.hashtags',\n",
    "                                   'entities.urls',\n",
    "                                   'entities.user_mentions',\n",
    "                                   'user.screen_name',\n",
    "                                   'user.created_at',\n",
    "                                   'user.followers_count']]\n",
    "original_tweets = original_tweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc524e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_tweets = retweeted_tweets[['id_str',\n",
    "                                     'user.id_str',\n",
    "                                     'retweeted_status.id_str',\n",
    "                                     'retweeted_status.created_at',\n",
    "                                     'retweeted_status.source',\n",
    "                                     'retweeted_status.text',\n",
    "                                     'retweeted_status.entities.hashtags',\n",
    "                                     'entities.user_mentions',\n",
    "                                     'retweeted_status.entities.urls',\n",
    "                                     'retweeted_status.entities.user_mentions',\n",
    "                                     'retweeted_status.user.created_at',\n",
    "                                     'retweeted_status.user.followers_count']]\n",
    "retweeted_tweets = retweeted_tweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8480aa4",
   "metadata": {},
   "source": [
    "Below we get the original author and his/her username for the retweets.\n",
    "Notice that we look up at the 'entities.user_mentions' column.\n",
    "In a retweet you can find the author of the original tweet as the first person mentioned in the 'entities.user_mentions' of the retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df18c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_author = {}\n",
    "for t in retweeted_tweets.index:\n",
    "    user_mentioned_json = retweeted_tweets[\"entities.user_mentions\"][t]\n",
    "    user_mentioned_loaded= json.loads(user_mentioned_json)\n",
    "    i=0\n",
    "    for screen_names in range(len(user_mentioned_loaded)):\n",
    "        if i==0 :\n",
    "            user_name_mentioned = user_mentioned_loaded[screen_names]['screen_name'] \n",
    "            user_mentioned_id = user_mentioned_loaded[screen_names]['id_str']\n",
    "            original_author.update({retweeted_tweets['id_str'][t]:(user_mentioned_id,user_name_mentioned)})\n",
    "            break\n",
    "#we break as soon as we find the first one cause thats the only one we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff24f3",
   "metadata": {},
   "source": [
    "We rename and rearrange the retweeted_tweets columns to match the original_tweets so we can merge them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_authors = pd.DataFrame(original_author.values(), columns=['user.id_str', 'user.screen_name'])\n",
    "retweeted_tweets.drop('id_str', axis=1, inplace=True)\n",
    "retweeted_tweets.drop('entities.user_mentions', axis=1, inplace=True)\n",
    "retweeted_tweets.drop('user.id_str', axis=1, inplace=True)\n",
    "retweeted_tweets = pd.concat([retweeted_tweets, Original_authors], axis=1,join='inner')\n",
    "retweeted_tweets = retweeted_tweets.rename(columns={'retweeted_status.id_str': 'id_str',\n",
    "                                                    'retweeted_status.created_at':'created_at',\n",
    "                                                    'retweeted_status.source':'source',\n",
    "                                                    'retweeted_status.text':'text',\n",
    "                                                    'retweeted_status.entities.hashtags':'entities.hashtags',\n",
    "                                                    'retweeted_status.entities.urls':'entities.urls',\n",
    "                                                    'retweeted_status.entities.user_mentions':'entities.user_mentions',\n",
    "                                                    'retweeted_status.user.created_at':'user.created_at',\n",
    "                                                    'retweeted_status.user.followers_count': 'user.followers_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd70ee32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retweeted_tweets = retweeted_tweets[['id_str',\n",
    "                                     'user.id_str',\n",
    "                                     'created_at',\n",
    "                                     'source',\n",
    "                                     'text', \n",
    "                                     'entities.hashtags',\n",
    "                                     'entities.urls',\n",
    "                                     'entities.user_mentions',\n",
    "                                     'user.screen_name',\n",
    "                                     'user.created_at',\n",
    "                                     'user.followers_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c413a6",
   "metadata": {},
   "source": [
    "We merge the 2 datasets into one and drop the duplicates that might exist. This could be the case cause the dataset could have an original tweet and its retweets but we only need one to create the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a55f4",
   "metadata": {},
   "source": [
    "Counts dataframe will be used to create usl and hashtagnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ea360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_final = pd.concat([original_tweets,retweeted_tweets],ignore_index=False)\n",
    "original_final.reset_index(drop=True,inplace=True)\n",
    "counts = original_final\n",
    "original_final = original_final.drop_duplicates(subset=['id_str'],keep = 'first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5eaf8",
   "metadata": {},
   "source": [
    "We use the py2neo.bulk to load our data fast. Our dataset is relatively small so just seeting a batch size is fine. If your dataset is larger you should set a lower batch size(1/5 of your data would be a good start) and then loop the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a42db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use bulk we need to send a list of dicts and thats what to_dict('records') does.\n",
    "unique_tweets = original_final.to_dict('records')\n",
    "stream = iter(unique_tweets)\n",
    "batch_size = 50000\n",
    "batch = islice(stream, batch_size)\n",
    "create_nodes(graph.auto(), batch, labels={\"Tweet\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be87bf",
   "metadata": {},
   "source": [
    "# Hashtag Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebc1a3",
   "metadata": {},
   "source": [
    "We create the Hashtage Nodes (Same logic will be applied in the Url Nodes too). We reform both counts dataframe and original_final datframe from json format to a list format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc22251",
   "metadata": {},
   "source": [
    "We could just do the reform on only the original_final. But there is a error in the dataframe that makes us lose hashtags and urls. Before dropping the duplicates in the original_final we find out that we have similar tweet ids with differnt hashtags/urls inside them. So we created the counts so we dont lose any valuable info in the form or hashtags/urls. You can see this as a work around to fix the error our data has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9e48cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/2892804214.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  counts['entities.hashtags'][t] = hash_list\n",
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/2892804214.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  original_final['entities.hashtags'][t] = hash_list\n"
     ]
    }
   ],
   "source": [
    "for t in counts.index:\n",
    "    hash_in_text = counts[\"entities.hashtags\"][t]\n",
    "    hash_list=[]\n",
    "    hashtags=json.loads(hash_in_text)\n",
    "    for k in range(len(hashtags)):\n",
    "        hash_list.append(hashtags[k]['text'].lower())\n",
    "    counts['entities.hashtags'][t] = hash_list\n",
    "for t in original_final.index:\n",
    "    hash_in_text = original_final[\"entities.hashtags\"][t]\n",
    "    hash_list=[]\n",
    "    hashtags=json.loads(hash_in_text)\n",
    "    for k in range(len(hashtags)):\n",
    "        hash_list.append(hashtags[k]['text'].lower())\n",
    "    original_final['entities.hashtags'][t] = hash_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3537e",
   "metadata": {},
   "source": [
    "Same as Tweet nodes we create the hashtag nodes after we use the unnest function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20764d18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/2324800661.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop(col, 1)\n"
     ]
    }
   ],
   "source": [
    "hashtag_node = counts[['entities.hashtags']]\n",
    "hashtag_node = hashtag_node[hashtag_node['entities.hashtags'].map(lambda d: len(d)) > 0]\n",
    "hashtag_node = unnest(hashtag_node, 'entities.hashtags')\n",
    "hashtag_node = hashtag_node.reset_index(drop=True)\n",
    "unique_hashtags = hashtag_node.drop_duplicates(subset=['entities.hashtags']).reset_index(drop=True)\n",
    "hashtag_dict = unique_hashtags.to_dict('records')\n",
    "stream = iter(hashtag_dict)\n",
    "batch_size = 50000\n",
    "batch = islice(stream, batch_size)\n",
    "create_nodes(graph.auto(), batch, labels={\"Hashtag\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d3e1d",
   "metadata": {},
   "source": [
    "# Url Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd5798c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/1921096117.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  counts['entities.urls'][t] = url_list\n",
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/1921096117.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  original_final['entities.urls'][t] = url_list\n"
     ]
    }
   ],
   "source": [
    "for t in counts.index:\n",
    "    url_in_text = counts[\"entities.urls\"][t]\n",
    "    url_list=[]\n",
    "    urls=json.loads(url_in_text)\n",
    "    for k in range(len(urls)):\n",
    "        url_list.append(urls[k][\"url\"])\n",
    "    counts['entities.urls'][t] = url_list\n",
    "for t in original_final.index:\n",
    "    url_in_text = original_final[\"entities.urls\"][t]\n",
    "    url_list=[]\n",
    "    urls=json.loads(url_in_text)\n",
    "    for k in range(len(urls)):\n",
    "        url_list.append(urls[k][\"url\"])\n",
    "    original_final['entities.urls'][t] = url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aefd87d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/2324800661.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop(col, 1)\n"
     ]
    }
   ],
   "source": [
    "url_node = counts[['entities.urls']]\n",
    "url_node = url_node[url_node['entities.urls'].map(lambda d: len(d)) > 0]\n",
    "url_node = unnest(url_node, 'entities.urls')\n",
    "url_node = url_node.reset_index(drop=True)\n",
    "unique_urls = url_node.drop_duplicates(subset=['entities.urls']).reset_index(drop=True)\n",
    "url_dict = unique_urls.to_dict('records')\n",
    "stream = iter(url_dict)\n",
    "batch_size = 50000\n",
    "batch = islice(stream, batch_size)\n",
    "create_nodes(graph.auto(), batch, labels={\"Url\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad5b15",
   "metadata": {},
   "source": [
    "# User Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba25d98",
   "metadata": {},
   "source": [
    "To get all the users we need to get the authors of original tweets, the users that retweeted tweets and all the mentioned users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44839e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_users_temp = {}\n",
    "for t in tweets.index:\n",
    "    user_mentioned_json = tweets[\"entities.user_mentions\"][t]\n",
    "    user_mentioned_loaded= json.loads(user_mentioned_json)\n",
    "    for screen_names in range(len(user_mentioned_loaded)):    \n",
    "        user_name_mentioned = user_mentioned_loaded[screen_names]['screen_name'] \n",
    "        user_mentioned_id = user_mentioned_loaded[screen_names]['id_str'] \n",
    "        temp_dict = {user_mentioned_id:user_name_mentioned}\n",
    "        mentioned_users_temp.update(temp_dict)\n",
    "#To make the loop faster  we use a dictionary and then we turn the dictionary in a dataframe.\n",
    "mentioned_users = pd.DataFrame(list(mentioned_users_temp.items()),columns = ['user.id_str','user.screen_name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dccedde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we merge all those 3 dataframes and then we drop duplicates.\n",
    "#The keep='first' is a crusial part so that we can keep the user with the most info (eg a mentioned use would not have a followers count)\n",
    "users = tweets[['user.id_str','user.screen_name','user.created_at','user.followers_count']]#tweets[['user.id_str','user.screen_name','user.created_at','user.followers_count']]\n",
    "users_rt = retweeted_tweets[['user.id_str','user.screen_name','user.created_at','user.followers_count']]\n",
    "original_tweets[['user.id_str','user.screen_name','user.created_at','user.followers_count']]\n",
    "final_users = pd.concat([users,users_rt, mentioned_users],ignore_index=False)\n",
    "final_users = final_users.drop_duplicates(subset=['user.id_str'],keep = 'first').reset_index(drop=True)\n",
    "unique_users = final_users.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8959c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = iter(unique_users)\n",
    "batch_size = 50000\n",
    "batch = islice(stream, batch_size)\n",
    "create_nodes(graph.auto(), batch, labels={\"User\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6ea59",
   "metadata": {},
   "source": [
    "# Relationship Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078beef7",
   "metadata": {},
   "source": [
    "To create the Tweet relationship between the user and the tweet (and all relationships in general) as fast as possible we need to create a list of tuples that has the form of (id of the first node,{relationship attributes},id of the other node). In this way the system doesn't search all the nodes to find the one that matches the criteria. On the other hand, this way works like a hash table (the node ids are unique) so the cost is considerably lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc584272",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher=NodeMatcher(g)\n",
    "nodes=matcher.match(\"Tweet\")\n",
    "master_dict_node = {}\n",
    "master_dict_rel = {}\n",
    "tweetid_nodeid = {}\n",
    "#parseing the nodes\n",
    "for node in nodes:\n",
    "    #gets the node id\n",
    "    nodeid = node.identity\n",
    "#for each node we get the respective attributes we are gonna need to create the relationship\n",
    "    tweet_id = (list(node.values())[4])\n",
    "    tweetid_nodeid.update({tweet_id:nodeid})\n",
    "    userid = (list(node.values())[0])\n",
    "    date = (list(node.values())[5])\n",
    "    source = (list(node.values())[6])\n",
    "#we create 2 dictionaries(we try to be fast and dictionaries and exceptionally fast in python)\n",
    "#master_dict_rel is a dictionary that contains the attributes of each relationship we need to create\n",
    "#the key is the id of the tweet node and the values are the date and source of this node\n",
    "    master_dict_rel.update({nodeid:(date,source)})\n",
    "#master_dict_node is dictionary that has a key in the form of the id of the node and value in the form of the id of the user created the tweet\n",
    "    master_dict_node.update({nodeid:userid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3417dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher=NodeMatcher(g)\n",
    "nodes=matcher.match(\"User\")\n",
    "master_user_dict = {}\n",
    "#here we create master_user_dict while parsing the nodes\n",
    "#master_user_dict is a dictionary that has a key in the form of the id of the user and value in the form of the id of the node that represents the user\n",
    "for node in nodes:\n",
    "    nodeid = node.identity\n",
    "    userid = (list(node.values())[0])\n",
    "    master_user_dict.update({userid:nodeid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b15e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "#we parse the master_dict_node dictionary and get the values. Each value we take we take we use to to access the master_user_dict and get the node id.\n",
    "#we also get the source and the date and create a small dictionary with them.\n",
    "for key in master_dict_node.keys():\n",
    "    a = master_dict_node.get(key)\n",
    "    b = master_user_dict.get(a)\n",
    "    c = master_dict_rel[key][0]\n",
    "    d = master_dict_rel[key][1]\n",
    "    temp_dict = {'Created at':c,'Source':d}\n",
    "    temp_tuple = (key,temp_dict,b)\n",
    "    test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe17c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Tweeted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b4b36",
   "metadata": {},
   "source": [
    "# Relationship Retweeted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048def07",
   "metadata": {},
   "source": [
    "The logic is like the previous relationship. \n",
    "The main difference is that we get only the tweets that were retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d5cbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_rt = tweets.dropna(subset = ['retweeted_status.id_str'])\n",
    "rel_rt = rel_rt[['id_str',\n",
    "                 'user.id_str',\n",
    "                 'retweeted_status.id_str',\n",
    "                 'retweeted_status.created_at',\n",
    "                 'retweeted_status.source',\n",
    "                 'retweeted_status.text',\n",
    "                 'retweeted_status.entities.hashtags',\n",
    "                 'entities.user_mentions',\n",
    "                 'retweeted_status.entities.urls',\n",
    "                 'retweeted_status.entities.user_mentions',\n",
    "                 'retweeted_status.user.created_at',\n",
    "                 'retweeted_status.user.followers_count']]\n",
    "rel_rt = rel_rt.reset_index(drop = True)\n",
    "Original_authors_rel = pd.DataFrame(original_author.values(), columns=['Author', 'Authot_name'])\n",
    "rel_rt = pd.concat([rel_rt, Original_authors_rel], axis=1,join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30c3b207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The master_rt_dict is a dictionary that has each retweet id as a key and as value the id of the original tweet and the user that retweeted the tweet.\n",
    "master_rt_dict = {}\n",
    "rt_rl_dict = {}\n",
    "for t in range(len(rel_rt)):\n",
    "    master_rt_dict.update({rel_rt['id_str'][t]:(rel_rt['retweeted_status.id_str'][t],rel_rt['user.id_str'][t])})\n",
    "    rt_rl_dict.update({rel_rt['id_str'][t]:(rel_rt['retweeted_status.user.created_at'][t],rel_rt['retweeted_status.source'][t])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a067c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as tweeted relationship we create the retweeted relationship\n",
    "test_list = []\n",
    "for key in master_rt_dict.keys():\n",
    "    id_str_og_tweet = master_rt_dict[key][0]\n",
    "    rt_user_id_str = master_rt_dict[key][1]\n",
    "    node_user_id = master_user_dict[rt_user_id_str]\n",
    "    node_tweet_id = tweetid_nodeid[id_str_og_tweet]\n",
    "    created = rt_rl_dict[key][0]\n",
    "    source = rt_rl_dict[key][1]\n",
    "    temp_dict = {'Created at':created,'Source':source}\n",
    "    temp_tuple = (node_tweet_id,temp_dict,node_user_id)\n",
    "    test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28fc93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168051\n"
     ]
    }
   ],
   "source": [
    "print(tweetid_nodeid[1.4947808842512507e+18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b37afcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Retweeted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d578a54",
   "metadata": {},
   "source": [
    "# Relationship Has Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d99c90",
   "metadata": {},
   "source": [
    "We find all the tweets that have a hashtag and create a dictionary with the id of the user that tweeted the tweet and a list with all the hashtags that the tweet contained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e23575",
   "metadata": {},
   "source": [
    "We also create a dictionary(hashtag_dict_node) that has the form of the hashtag name as the key and the node id that represents that hashtag as a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e44f310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_dict = {}\n",
    "for t in range(len(original_final)):\n",
    "    if len(original_final['entities.hashtags'][t])!=0:\n",
    "        hashtag_dict.update({original_final['id_str'][t]:original_final['entities.hashtags'][t]})\n",
    "matcher=NodeMatcher(g)\n",
    "nodes=matcher.match(\"Hashtag\")\n",
    "hashtag_dict_node = {}\n",
    "for node in nodes:\n",
    "    nodeid = node.identity\n",
    "    hashtagid = (list(node.values())[0])\n",
    "    hashtag_dict_node.update({hashtagid:nodeid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0385ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "temp_dict = {}\n",
    "for key in hashtag_dict.keys():\n",
    "    a = len(hashtag_dict[key])\n",
    "#for each value in hashtag_dict_node so for each hashtag we get the node that this hashtag is located.\n",
    "#after that we with the tweet id the node that this tweet represents through tweetid_nodeid\n",
    "#and like above we create a a tuple and pass it to a list so we can create the relationship\n",
    "    for i in range(a):\n",
    "        hashid = hashtag_dict_node[hashtag_dict[key][i]]\n",
    "        tweetid = tweetid_nodeid[key]\n",
    "        temp_tuple = (hashid,temp_dict,tweetid)\n",
    "        test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f55afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Has Hashtag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96597e7",
   "metadata": {},
   "source": [
    "# Relationship Has Url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0c451",
   "metadata": {},
   "source": [
    "Same technique as avove in the has hashtag relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22bc57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "for t in range(len(original_final)):\n",
    "    if len(original_final['entities.urls'][t])!=0:\n",
    "        url_dict.update({original_final['id_str'][t]:original_final['entities.urls'][t]})\n",
    "matcher=NodeMatcher(g)\n",
    "nodes=matcher.match(\"Url\")\n",
    "url_dict_node = {}\n",
    "for node in nodes:\n",
    "    nodeid = node.identity\n",
    "    urlid = (list(node.values())[0])\n",
    "    url_dict_node.update({urlid:nodeid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8347352",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "temp_dict = {}\n",
    "for key in url_dict.keys():\n",
    "    a = len(url_dict[key])\n",
    "    for i in range(a):\n",
    "        urlid = url_dict_node[url_dict[key][i]]\n",
    "        tweetid = tweetid_nodeid[key]\n",
    "        temp_tuple = (urlid,temp_dict,tweetid)\n",
    "        test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfa3c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Has Url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03f932",
   "metadata": {},
   "source": [
    "# Relationship Used Hashtag \n",
    "## (for normal tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5159482",
   "metadata": {},
   "source": [
    "For this  relationship we need to break our data into 2 parts. One for the normaly tweeted tweets and one for the retweeted ones. The same logic applies to both of them: the only thing that changes is the dataframe that it is applied on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32f6cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_dict = {}\n",
    "userid_dict = {}\n",
    "test_list = []\n",
    "temp_dict = {}\n",
    "for t in range(len(original_final)):\n",
    "    if len(original_final['entities.hashtags'][t])!=0:\n",
    "#as we have done countless times before we create dictionaries that have as a key the id of the tweet and as values the user id on the first one and the hashtags as a list in the second one\n",
    "        hashtag_dict.update({original_final['id_str'][t]:original_final['user.id_str'][t]})\n",
    "        userid_dict.update({original_final['id_str'][t]:original_final['entities.hashtags'][t]})\n",
    "#same logic as has hashtag relationship\n",
    "for key in hashtag_dict:\n",
    "    user_node_id = master_user_dict[hashtag_dict[key]]\n",
    "    for i in range(len(userid_dict[key])):\n",
    "        b = userid_dict[key][i]\n",
    "        hash_node_id = hashtag_dict_node[b]\n",
    "        temp_tuple = (user_node_id,temp_dict,hash_node_id)\n",
    "        test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38eb795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Used Hashtag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5713d9",
   "metadata": {},
   "source": [
    "# Relationship Used Hashtag \n",
    "## (for retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e202405e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/2282643530.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rel_rt['retweeted_status.entities.hashtags'][t] = hash_list\n"
     ]
    }
   ],
   "source": [
    "for t in rel_rt.index:\n",
    "    hash_in_text = rel_rt['retweeted_status.entities.hashtags'][t]\n",
    "    hash_list=[]\n",
    "    hashtags=json.loads(hash_in_text)\n",
    "    for k in range(len(hashtags)):\n",
    "        hash_list.append(hashtags[k]['text'].lower())\n",
    "    rel_rt['retweeted_status.entities.hashtags'][t] = hash_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59de9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_dict = {}\n",
    "userid_dict = {}\n",
    "test_list = []\n",
    "temp_dict = {}\n",
    "for t in range(len(rel_rt)):\n",
    "    if len(rel_rt['retweeted_status.entities.hashtags'][t])!=0:\n",
    "        hashtag_dict.update({rel_rt['id_str'][t]:rel_rt['user.id_str'][t]})\n",
    "        userid_dict.update({rel_rt['id_str'][t]:rel_rt['retweeted_status.entities.hashtags'][t]})\n",
    "for key in hashtag_dict:\n",
    "    user_node_id = master_user_dict[hashtag_dict[key]]\n",
    "    for i in range(len(userid_dict[key])):\n",
    "        b = userid_dict[key][i]\n",
    "        hash_node_id = hashtag_dict_node[b]\n",
    "        temp_tuple = (user_node_id,temp_dict,hash_node_id)\n",
    "        test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b861b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Used Hashtag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7365605",
   "metadata": {},
   "source": [
    "# Relationship Used Url\n",
    "## (for Normal Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775cebc",
   "metadata": {},
   "source": [
    "Like we did for the Has Hashtag relationship for both tweets and retweets, we do the same for the url part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49a325c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "userid_dict = {}\n",
    "test_list = []\n",
    "temp_dict = {}\n",
    "for t in range(len(original_final)):\n",
    "    if len(original_final['entities.urls'][t])!=0:\n",
    "        url_dict.update({original_final['id_str'][t]:original_final['user.id_str'][t]})\n",
    "        userid_dict.update({original_final['id_str'][t]:original_final['entities.urls'][t]})\n",
    "for key in url_dict:\n",
    "    user_node_id = master_user_dict[url_dict[key]]\n",
    "    for i in range(len(userid_dict[key])):\n",
    "        b = userid_dict[key][i]\n",
    "        url_node_id = url_dict_node[b]\n",
    "        temp_tuple = (user_node_id,temp_dict,hash_node_id)\n",
    "        test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "898c4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Used Url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04a1ad",
   "metadata": {},
   "source": [
    "# Relationship Used Url\n",
    "## (For Normal Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38caf101",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xaris\\AppData\\Local\\Temp/ipykernel_22780/648739987.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rel_rt['retweeted_status.entities.urls'][t] = url_list\n"
     ]
    }
   ],
   "source": [
    "for t in rel_rt.index:\n",
    "    url_in_text = rel_rt[\"retweeted_status.entities.urls\"][t]\n",
    "    url_list=[]\n",
    "    urls=json.loads(url_in_text)\n",
    "    for k in range(len(urls)):\n",
    "        url_list.append(urls[k][\"url\"])\n",
    "    rel_rt['retweeted_status.entities.urls'][t] = url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e292d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_dict = {}\n",
    "url_dict = {}\n",
    "test_list = []\n",
    "temp_dict = {}\n",
    "for t in range(len(rel_rt)):\n",
    "    if len(rel_rt['retweeted_status.entities.urls'][t])!=0:\n",
    "        url_dict.update({rel_rt['id_str'][t]:rel_rt['user.id_str'][t]})\n",
    "        userid_dict.update({rel_rt['id_str'][t]:rel_rt['retweeted_status.entities.urls'][t]})\n",
    "for key in url_dict:\n",
    "    user_node_id = master_user_dict[url_dict[key]]\n",
    "    for i in range(len(userid_dict[key])):\n",
    "        b = userid_dict[key][i]\n",
    "        url_node_id = url_dict_node[b]\n",
    "        temp_tuple = (user_node_id,temp_dict,hash_node_id)\n",
    "        test_list.append(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8528eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Used Url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae929b23",
   "metadata": {},
   "source": [
    "# Mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9862d24",
   "metadata": {},
   "source": [
    "Now for the last part we need to find and connect who mentioned who."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b938f",
   "metadata": {},
   "source": [
    "Just like the has hashtag relationship we have 2 loops one inside the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f8bac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_users\n",
    "master_user_dict\n",
    "mentions = tweets[['user.id_str','entities.user_mentions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "849e9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "temp_dict = {}\n",
    "for i in mentions.index:\n",
    "#for each user we refer to the dictionaries we have created before.\n",
    "# Reform the json as we did before\n",
    "    current_user = mentions['user.id_str'][i]\n",
    "    a = master_user_dict[current_user]\n",
    "    user_mentioned_json = mentions['entities.user_mentions'][i]\n",
    "    user_mentioned_loaded= json.loads(user_mentioned_json)\n",
    "    for screen_names in range(len(user_mentioned_loaded)):\n",
    "#for each user we refer to master_user_dict to get the node id \n",
    "#this way we get both users node id so we create the tuple\n",
    "        user_mentioned_id = user_mentioned_loaded[screen_names]['id_str']\n",
    "        b = master_user_dict[user_mentioned_id]\n",
    "        temp_tuple = (a,temp_dict,b)\n",
    "        test_list.append(temp_tuple)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2aec1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_relationships(g.auto(), test_list, \"Mention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4806cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.67172050476074\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afcfbe",
   "metadata": {},
   "source": [
    "The shown time is from a pc that uses an i74790k(2014 cpu) and 16bg of ddr3 1600hz memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
